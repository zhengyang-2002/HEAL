{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd39de4f-8c0a-42da-b46b-ba312d6e9800",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tools\n",
    "import torch\n",
    "import Prompt\n",
    "from openai import OpenAI\n",
    "from definition import ModelLoader\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, PeftConfig\n",
    "from transformers import LlamaForCausalLM, pipeline, LlamaTokenizer, AutoTokenizer, AutoModel, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0825583-884c-455c-b85c-29a0b5a9b3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已分配至空闲GPU：cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52fbbe639efa4d36bceb56559bfca1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在合并Lora结构，路径:/root/LLaMA-Factory/src/saves/Qwen2-7B/lora/2st/checkpoint-1714/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已分配至空闲GPU：cuda:1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d42c88a1d71c4e86be2ff6dd85fbd45b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "patient=ModelLoader('/root/autodl-tmp/weights/Qwen2-7B/', '/root/LLaMA-Factory/src/saves/Qwen2-7B/lora/2st/checkpoint-1714/', system_prompt='你是一个心理问题求助者')\n",
    "soulchat=ModelLoader('/root/autodl-tmp/weights/SoulChat2.0-Llama-3.1-8B', system_prompt='你是一个杰出的心理治疗师')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9949329f-0ed5-4d9e-9b03-de88efa0e42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 进行 3 轮对话\n",
    "dialogue = tools.conduct_dialogue(patient, soulcha, rounds=10)\n",
    "\n",
    "# 打印对话记录\n",
    "for line in dialogue:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69362e7c-70dd-4503-b123-95f2b55709a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_dialogue(patient: ModelLoader, doctor: ModelLoader, rounds: int) -> list:\n",
    "    \"\"\"\n",
    "    让求助者和心理医生进行对话，并记录对话过程。\n",
    "\n",
    "    :param patient: 扮演求助者的 ModelLoader 实例\n",
    "    :param doctor: 扮演心理医生的 ModelLoader 实例\n",
    "    :param rounds: 对话的轮次\n",
    "    :return: 记录对话过程的列表，格式为 [<求助者>, <心理医生>, <求助者>, <心理医生>, ...]\n",
    "    \"\"\"\n",
    "    dialogue_history = []  # 用于记录对话过程\n",
    "    doctor_reply = \"\"  # 初始输入为空\n",
    "\n",
    "    for _ in range(rounds):\n",
    "        # 求助者发言\n",
    "        patient_query = patient.chat(doctor_reply)\n",
    "        dialogue_history.append(f\"<求助者>{patient_query}\")\n",
    "\n",
    "        # 心理医生回复\n",
    "        doctor_reply = doctor.chat(patient_query)\n",
    "        dialogue_history.append(f\"<心理医生>{doctor_reply}\")\n",
    "\n",
    "    return dialogue_history\n",
    "\n",
    "\n",
    "llama_3=ModelLoader('/root/autodl-tmp/weights/llama3.1-8B-chat', '现在请你扮演一个有心理问题，需要帮助的心理求助者，你正在和心理医生对话。请直接描述你的心理问题，不要透露你现在正在扮演，不要说无关心理咨询的话。')\n",
    "qwen_2=ModelLoader('/root/autodl-tmp/weights/Qwen2-7B-Chat', '你现在是一个心理医生，正在和患者进行对话')\n",
    "\n",
    "%%time\n",
    "# 进行 3 轮对话\n",
    "dialogue = conduct_dialogue(llama_3, qwen_2, rounds=10)\n",
    "\n",
    "# 打印对话记录\n",
    "for line in dialogue:\n",
    "    print(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
